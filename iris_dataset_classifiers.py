# -*- coding: utf-8 -*-
"""Iris_dataset_classifiers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1De9mW9xFR0sMiAQvc08IzTQxe2_htH8J
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier


import numpy as np
import matplotlib.pyplot as plt

svm_clf = SVC()
dt_clf = DecisionTreeClassifier()
log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()

clf = [svm_clf, dt_clf, log_clf, rnd_clf]

arr = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]

iris = load_iris()
plt.figure(figsize=(14,9))

for ot,i in enumerate(arr):
  X = load_iris().data[:, i]
  Y = load_iris().target
  
  plt.subplot(2,3, ot+1)
  

  
  # this formatter will label the colorbar with the correct target names
  formatter = plt.FuncFormatter(lambda j, *args: iris.target_names[int(j)])
  
  plt.scatter(X[:, 0], X[:, 1], c=iris.target)
  plt.colorbar(ticks=[0, 1, 2], format=formatter)
  
  plt.xlabel(iris.feature_names[i[0]], color="white")
  plt.ylabel(iris.feature_names[i[1]], color="white")
  
plt.show()

for classifier in clf:
  # Parameters
  n_classes = 3
  plot_colors = ["mediumslateblue", "pink", "lime"]
  scatter_colors = ["navy", "firebrick", "darkgreen"]
  plot_step = 0.02

  # Load data
  iris = load_iris()

  plt.figure(figsize=(14,9))

  for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],
                                  [1, 2], [1, 3], [2, 3]]):
      # We only take the two corresponding features
      X = iris.data[:, pair]
      y = iris.target
      
      x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7)
      
      # Train
      clf = classifier.fit(x_train, y_train)
      y_pred = clf.predict(x_test)
      

      # Plot the decision boundary
      sub =  plt.subplot(2, 3, pairidx + 1)
      
      

      x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
      y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
      xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                           np.arange(y_min, y_max, plot_step))

      Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
      Z = Z.reshape(xx.shape)
      
      acc = accuracy_score(y_test, y_pred)
      sub.set_title("Accuracy :- "+str(acc), color="lime")
      
      cs = plt.contourf(xx, yy, Z, alpha=0.5)


      plt.xlabel(iris.feature_names[pair[0]])
      plt.ylabel(iris.feature_names[pair[1]])
      plt.axis("tight")

      # Plot the training points
      for i, color in zip(range(n_classes), scatter_colors):
          idx = np.where(y == i)
          plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],
                      cmap=plt.cm.Paired, alpha=1)

      plt.axis("tight")

  plt.suptitle("Decision surface of a "+ classifier.__class__.__name__ +" using paired features with accuracy of ", color="red")
  plt.legend()
  plt.show()